{
 "cells": [
  {
   "cell_type": "code",
   "id": "4b82e32c11f0fc62",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T14:13:37.429237Z",
     "start_time": "2025-03-25T14:13:37.424729Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from skimage import color"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:13:37.440747Z",
     "start_time": "2025-03-25T14:13:37.435735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_harris_corners(image_path, output_path):\n",
    "    \"\"\"\n",
    "    对输入图像进行 Harris 角点检测，并在角点位置标记红色后保存结果\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray_f = np.float32(gray)\n",
    "    harris_response = cv2.cornerHarris(gray_f, blockSize=2, ksize=3, k=0.04)\n",
    "    harris_response = cv2.dilate(harris_response, None)\n",
    "    img[harris_response > 0.01 * harris_response.max()] = [0, 0, 255]\n",
    "    cv2.imwrite(output_path, img)"
   ],
   "id": "ff595e7cbc255fa5",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:13:37.453407Z",
     "start_time": "2025-03-25T14:13:37.448995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_sift_features(image_path):\n",
    "    \"\"\"\n",
    "    使用 SIFT 提取图像的关键点和描述子\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "    return keypoints, descriptors"
   ],
   "id": "e8503d6c63b03096",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:13:37.468671Z",
     "start_time": "2025-03-25T14:13:37.462457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_hog_features(image_path, patch_size=32):\n",
    "    \"\"\"\n",
    "    利用 Harris 算法检测角点，然后对每个角点周围提取固定大小（patch_size x patch_size）的图像块，\n",
    "    并计算 HOG 描述子，返回关键点（cv2.KeyPoint 列表）和描述子（numpy 数组）。\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    gray_f = np.float32(img)\n",
    "    harris_response = cv2.cornerHarris(gray_f, blockSize=2, ksize=3, k=0.04)\n",
    "    harris_response = cv2.dilate(harris_response, None)\n",
    "    threshold = 0.01 * harris_response.max()\n",
    "    # 获取角点坐标 (y, x)\n",
    "    coords = np.argwhere(harris_response > threshold)\n",
    "    \n",
    "    keypoints = []\n",
    "    descriptors = []\n",
    "    half = patch_size // 2\n",
    "    for (y, x) in coords:\n",
    "        # 确保图像块在图像内\n",
    "        if x - half < 0 or x + half >= img.shape[1] or y - half < 0 or y + half >= img.shape[0]:\n",
    "            continue\n",
    "        patch = img[y-half:y+half, x-half:x+half]\n",
    "        hog_desc = hog(patch, pixels_per_cell=(8,8), cells_per_block=(2,2), feature_vector=True)\n",
    "        descriptors.append(hog_desc)\n",
    "        # 转换 x,y 为 float 类型构造 cv2.KeyPoint 对象\n",
    "        keypoints.append(cv2.KeyPoint(float(x), float(y), patch_size))\n",
    "    \n",
    "    if len(descriptors) == 0:\n",
    "        descriptors = None\n",
    "    else:\n",
    "        descriptors = np.array(descriptors, dtype=np.float32)\n",
    "        # 如果只有一个角点，则确保 descriptors 为二维数组\n",
    "        if descriptors.ndim == 1:\n",
    "            descriptors = descriptors[np.newaxis, :]\n",
    "    return keypoints, descriptors\n",
    "\n"
   ],
   "id": "fb36985aa81a6ab2",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:13:37.479969Z",
     "start_time": "2025-03-25T14:13:37.476115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def match_features(desc1, desc2):\n",
    "    \"\"\"\n",
    "    使用暴力匹配器对两组描述子进行匹配，并返回按距离排序的匹配结果\n",
    "    \"\"\"\n",
    "    # 确保 descriptor 为连续的 numpy 数组\n",
    "    desc1 = np.ascontiguousarray(desc1, dtype=np.float32)\n",
    "    desc2 = np.ascontiguousarray(desc2, dtype=np.float32)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(desc1, desc2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    return matches"
   ],
   "id": "f4713a5864c1b4d4",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:13:37.499559Z",
     "start_time": "2025-03-25T14:13:37.493843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_and_match_features(img1_path, img2_path, method, match_output_path):\n",
    "    \"\"\"\n",
    "    根据所选描述子类型（SIFT 或 HOG），提取图像的关键点与描述子，然后匹配，\n",
    "    并将匹配结果绘制后保存到指定路径。\n",
    "    \"\"\"\n",
    "    if method.upper() == \"SIFT\":\n",
    "        kp1, desc1 = extract_sift_features(img1_path)\n",
    "        kp2, desc2 = extract_sift_features(img2_path)\n",
    "    elif method.upper() == \"HOG\":\n",
    "        kp1, desc1 = extract_hog_features(img1_path)\n",
    "        kp2, desc2 = extract_hog_features(img2_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Please use 'SIFT' or 'HOG'.\")\n",
    "    \n",
    "    if desc1 is None or desc2 is None or len(kp1) == 0 or len(kp2) == 0:\n",
    "        raise ValueError(\"No descriptors found in one of the images.\")\n",
    "    \n",
    "    matches = match_features(desc1, desc2)\n",
    "    \n",
    "    img1 = cv2.imread(img1_path)\n",
    "    img2 = cv2.imread(img2_path)\n",
    "    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:50], None, flags=2)\n",
    "    cv2.imwrite(match_output_path, img_matches)"
   ],
   "id": "b65387f8183136e",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:11:04.164016Z",
     "start_time": "2025-03-25T15:11:04.148289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stitch_images(img1_path, img2_path, method, output_path):\n",
    "    \"\"\"\n",
    "    使用 SIFT 或 HOG 提取关键点并匹配，利用 RANSAC 计算仿射变换矩阵，将第二张图拼接到第一张图上。\n",
    "    该版本支持自动扩展画布，避免错位问题。\n",
    "    \"\"\"\n",
    "    if method.upper() == \"SIFT\":\n",
    "        kp1, desc1 = extract_sift_features(img1_path)\n",
    "        kp2, desc2 = extract_sift_features(img2_path)\n",
    "    elif method.upper() == \"HOG\":\n",
    "        kp1, desc1 = extract_hog_features(img1_path)\n",
    "        kp2, desc2 = extract_hog_features(img2_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Please use 'SIFT' or 'HOG'.\")\n",
    "\n",
    "    if desc1 is None or desc2 is None or len(kp1) == 0 or len(kp2) == 0:\n",
    "        raise ValueError(\"No descriptors found in one of the images.\")\n",
    "\n",
    "    matches = match_features(desc1, desc2)\n",
    "    if len(matches) < 3:\n",
    "        raise ValueError(\"Not enough matches to compute transformation.\")\n",
    "\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    M, mask = cv2.estimateAffinePartial2D(src_pts, dst_pts, method=cv2.RANSAC)\n",
    "\n",
    "    img1 = cv2.imread(img1_path)\n",
    "    img2 = cv2.imread(img2_path)\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "\n",
    "    corners_img2 = np.array([\n",
    "        [0, 0], [w2, 0], [w2, h2], [0, h2]\n",
    "    ], dtype=np.float32).reshape(-1, 1, 2)\n",
    "    transformed_corners = cv2.transform(corners_img2, M)\n",
    "\n",
    "    # 合并所有相关角点\n",
    "    img1_corners = np.array([\n",
    "        [0, 0], [w1, 0], [w1, h1], [0, h1]\n",
    "    ], dtype=np.float32).reshape(-1, 1, 2)\n",
    "    all_corners = np.concatenate(\n",
    "        (transformed_corners, img1_corners),\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    x_coords = all_corners[:, :, 0]\n",
    "    y_coords = all_corners[:, :, 1]\n",
    "    x_min, x_max = np.floor(np.min(x_coords)), np.ceil(np.max(x_coords))\n",
    "    y_min, y_max = np.floor(np.min(y_coords)), np.ceil(np.max(y_coords))\n",
    "    output_width = int(x_max - x_min)\n",
    "    output_height = int(y_max - y_min)\n",
    "\n",
    "    # 构建平移矩阵\n",
    "    translation_matrix = np.array([\n",
    "        [1, 0, -x_min],\n",
    "        [0, 1, -y_min],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    M = np.vstack([M, [0, 0, 1]])\n",
    "\n",
    "    transformation_matrix = translation_matrix @ M\n",
    "    transformation_matrix = transformation_matrix[:2, :]\n",
    "\n",
    "    warped_img1 = cv2.warpAffine(img1, transformation_matrix, (output_width, output_height))\n",
    "    warped_img2 = cv2.warpAffine(img2, translation_matrix[:2, :], (output_width, output_height))\n",
    "\n",
    "    # 为每个图像创建掩膜（非零像素设置为1）\n",
    "    mask1 = (warped_img1.sum(axis=2) > 0).astype(np.uint8)\n",
    "    mask2 = (warped_img2.sum(axis=2) > 0).astype(np.uint8)\n",
    "\n",
    "    # 利用距离变换计算每个像素离边缘的距离\n",
    "    dist1 = cv2.distanceTransform(mask1, cv2.DIST_L2, 5)\n",
    "    dist2 = cv2.distanceTransform(mask2, cv2.DIST_L2, 5)\n",
    "\n",
    "    # 防止除零错误，加入极小值 epsilon\n",
    "    epsilon = 1e-5\n",
    "    combined = dist1 + dist2 + epsilon\n",
    "    w1 = dist1 / combined\n",
    "    w2 = dist2 / combined\n",
    "\n",
    "    # 扩展权重通道以便与图像对应\n",
    "    w1 = np.expand_dims(w1, axis=2)\n",
    "    w2 = np.expand_dims(w2, axis=2)\n",
    "\n",
    "    # 转换图像为 float 类型用于加权计算\n",
    "    warped_img1_f = warped_img1.astype(np.float32)\n",
    "\n",
    "    # 初始化结果图像\n",
    "    result = np.zeros_like(warped_img1_f)\n",
    "\n",
    "    # 确定重叠区域\n",
    "    overlap = np.logical_and(mask1, mask2).astype(bool)\n",
    "\n",
    "    # 提取重叠区域的亮度通道（转为HSV空间）\n",
    "    hsv1 = cv2.cvtColor(warped_img1, cv2.COLOR_BGR2HSV)\n",
    "    hsv2 = cv2.cvtColor(warped_img2, cv2.COLOR_BGR2HSV)\n",
    "    v1 = hsv1[overlap, 2].astype(np.float32)\n",
    "    v2 = hsv2[overlap, 2].astype(np.float32)\n",
    "\n",
    "    # 计算亮度差异的缩放因子（避免除以零）\n",
    "    mean_v1 = np.mean(v1) + 1e-5\n",
    "    mean_v2 = np.mean(v2) + 1e-5\n",
    "    scale_factor = mean_v1 / mean_v2\n",
    "\n",
    "    # 对图像2的亮度通道进行校正\n",
    "    hsv2[:, :, 2] = np.clip(hsv2[:, :, 2] * scale_factor, 0, 255).astype(np.uint8)\n",
    "    warped_img2 = cv2.cvtColor(hsv2, cv2.COLOR_HSV2BGR)\n",
    "    warped_img2_f = warped_img2.astype(np.float32)\n",
    "\n",
    "    # 对权重进行高斯模糊（核大小设为15，标准差3）\n",
    "    w1_blur = cv2.GaussianBlur(w1, (15, 15), 3)\n",
    "    w2_blur = cv2.GaussianBlur(w2, (15, 15), 3)\n",
    "\n",
    "    # 重新归一化权重（防止总和超过1）\n",
    "    sum_weights = w1_blur + w2_blur + 1e-5\n",
    "    w1_final = w1_blur / sum_weights\n",
    "    w2_final = w2_blur / sum_weights\n",
    "    w1_final = np.expand_dims(w1_final, axis=2)  # 形状从 (H,W) 变为 (H,W,1)\n",
    "    w2_final = np.expand_dims(w2_final, axis=2)\n",
    "\n",
    "    # 重叠区域使用模糊后的权重融合\n",
    "    result[overlap] = warped_img1_f[overlap] * w1_final[overlap] + warped_img2_f[overlap] * w2_final[overlap]\n",
    "\n",
    "    # 非重叠区域使用原图，但添加过渡（避免硬边界）\n",
    "    non_overlap1 = np.logical_and(mask1, ~overlap)\n",
    "    non_overlap2 = np.logical_and(mask2, ~overlap)\n",
    "\n",
    "    # 对非重叠区域边缘进行轻微模糊\n",
    "    mask1_blur = cv2.GaussianBlur(mask1.astype(np.float32), (5,5), 1)\n",
    "    mask2_blur = cv2.GaussianBlur(mask2.astype(np.float32), (5,5), 1)\n",
    "    result[non_overlap1] = warped_img1_f[non_overlap1] * mask1_blur[non_overlap1, None]\n",
    "    result[non_overlap2] += warped_img2_f[non_overlap2] * mask2_blur[non_overlap2, None]\n",
    "\n",
    "    # 转换回 uint8 类型并输出结果\n",
    "    result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "    cv2.imwrite(output_path, result)"
   ],
   "id": "388fb2e471bcd153",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:30:06.328104Z",
     "start_time": "2025-03-25T14:30:06.323612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stitch_multiple_images(image_paths, output_path):\n",
    "    \"\"\"\n",
    "    基于 SIFT + RANSAC 的方法，依次对多幅图像进行拼接，\n",
    "    最终将拼接结果保存到 output_path 中。\n",
    "    \"\"\"\n",
    "    if len(image_paths) < 2:\n",
    "        raise ValueError(\"Need at least 2 images\")\n",
    "\n",
    "    # 初始化基准图像\n",
    "    base_img = cv2.imread(image_paths[0])\n",
    "    cv2.imwrite(output_path, base_img)\n",
    "\n",
    "    # 逐个拼接后续图像\n",
    "    for img_path in image_paths[1:]:\n",
    "        try:\n",
    "            stitch_images(output_path, img_path, \"SIFT\", output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {img_path} due to error: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 保存最终结果\n",
    "    final_result = cv2.imread(output_path)\n",
    "    cv2.imwrite(output_path, final_result)\n"
   ],
   "id": "3fefd32a636c766e",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:13:37.558912Z",
     "start_time": "2025-03-25T14:13:37.534058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 检测角点\n",
    "detect_harris_corners(\"images/sudoku.png\", \"results/sudoku_keypoints.png\")"
   ],
   "id": "1a4192e2b256e926",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:13:37.675801Z",
     "start_time": "2025-03-25T14:13:37.566011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 提取 SIFT 特征\n",
    "extract_and_match_features(\"images/uttower1.jpg\", \"images/uttower2.jpg\", \"SIFT\", \"results/uttower_match_sift.png\")"
   ],
   "id": "d7c4754e09cfc156",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:13:52.226117Z",
     "start_time": "2025-03-25T14:13:37.682013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 提取 HOG 特征\n",
    "extract_and_match_features(\"images/uttower1.jpg\", \"images/uttower2.jpg\", \"HOG\", \"results/uttower_match_hog.png\")"
   ],
   "id": "83ff00e705ccdca6",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:11:26.491637Z",
     "start_time": "2025-03-25T15:11:07.704593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 拼接两张图像\n",
    "stitch_images(\"images/uttower1.jpg\", \"images/uttower2.jpg\", \"SIFT\", \"results/uttower_stitching_sift.png\")\n",
    "stitch_images(\"images/uttower1.jpg\", \"images/uttower2.jpg\", \"HOG\", \"results/uttower_stitching_hog.png\")"
   ],
   "id": "3f826650ab5dc4c8",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:11:28.792873Z",
     "start_time": "2025-03-25T15:11:26.503400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 拼接多张图像\n",
    "stitch_multiple_images([\"images/yosemite1.jpg\", \"images/yosemite2.jpg\", \"images/yosemite3.jpg\", \"images/yosemite4.jpg\"], \"results/yosemite_stitching.png\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 75
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
